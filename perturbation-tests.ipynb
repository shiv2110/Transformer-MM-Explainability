{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8552460,"sourceType":"datasetVersion","datasetId":5110830}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-31T05:01:43.533280Z","iopub.execute_input":"2024-05-31T05:01:43.534068Z","iopub.status.idle":"2024-05-31T05:01:43.540134Z","shell.execute_reply.started":"2024-05-31T05:01:43.534034Z","shell.execute_reply":"2024-05-31T05:01:43.539165Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/shiv2110/Transformer-MM-Explainability","metadata":{"execution":{"iopub.status.busy":"2024-06-01T08:49:00.797640Z","iopub.execute_input":"2024-06-01T08:49:00.798306Z","iopub.status.idle":"2024-06-01T08:49:05.510817Z","shell.execute_reply.started":"2024-06-01T08:49:00.798277Z","shell.execute_reply":"2024-06-01T08:49:05.509836Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'Transformer-MM-Explainability'...\nremote: Enumerating objects: 2225, done.\u001b[K\nremote: Counting objects: 100% (651/651), done.\u001b[K\nremote: Compressing objects: 100% (364/364), done.\u001b[K\nremote: Total 2225 (delta 307), reused 572 (delta 257), pack-reused 1574\u001b[K\nReceiving objects: 100% (2225/2225), 49.39 MiB | 28.49 MiB/s, done.\nResolving deltas: 100% (990/990), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"# !ls /kaggle/input","metadata":{"execution":{"iopub.status.busy":"2024-06-01T08:49:05.513039Z","iopub.execute_input":"2024-06-01T08:49:05.513407Z","iopub.status.idle":"2024-06-01T08:49:05.518449Z","shell.execute_reply.started":"2024-06-01T08:49:05.513371Z","shell.execute_reply":"2024-06-01T08:49:05.517352Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%cd Transformer-MM-Explainability","metadata":{"execution":{"iopub.status.busy":"2024-06-01T08:49:05.519465Z","iopub.execute_input":"2024-06-01T08:49:05.519701Z","iopub.status.idle":"2024-06-01T08:49:05.530719Z","shell.execute_reply.started":"2024-06-01T08:49:05.519680Z","shell.execute_reply":"2024-06-01T08:49:05.529689Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/working/Transformer-MM-Explainability\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pwd","metadata":{"execution":{"iopub.status.busy":"2024-06-01T08:49:05.532649Z","iopub.execute_input":"2024-06-01T08:49:05.533304Z","iopub.status.idle":"2024-06-01T08:49:06.518535Z","shell.execute_reply.started":"2024-06-01T08:49:05.533273Z","shell.execute_reply":"2024-06-01T08:49:06.517555Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/working/Transformer-MM-Explainability\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2024-06-01T08:49:06.752802Z","iopub.execute_input":"2024-06-01T08:49:06.753184Z","iopub.status.idle":"2024-06-01T08:49:34.243685Z","shell.execute_reply.started":"2024-06-01T08:49:06.753151Z","shell.execute_reply":"2024-06-01T08:49:34.242706Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: Pillow>=8.1.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (9.5.0)\nCollecting einops==0.3.0 (from -r requirements.txt (line 2))\n  Downloading einops-0.3.0-py2.py3-none-any.whl.metadata (10 kB)\nCollecting imageio==2.9.0 (from -r requirements.txt (line 3))\n  Downloading imageio-2.9.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (3.7.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.26.4)\nRequirement already satisfied: opencv_python in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.9.0.80)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.11.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.2.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.16.2)\nCollecting tqdm==4.51.0 (from -r requirements.txt (line 11))\n  Downloading tqdm-4.51.0-py2.py3-none-any.whl.metadata (55 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting transformers==4.31.0 (from -r requirements.txt (line 12))\n  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting utils==1.0.1 (from -r requirements.txt (line 13))\n  Downloading utils-1.0.1-py2.py3-none-any.whl.metadata (4.6 kB)\nCollecting captum (from -r requirements.txt (line 14))\n  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\nCollecting wget (from -r requirements.txt (line 15))\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting ftfy (from -r requirements.txt (line 16))\n  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (2023.12.25)\nCollecting pymatting (from -r requirements.txt (line 18))\n  Downloading PyMatting-1.1.12-py3-none-any.whl.metadata (7.4 kB)\nRequirement already satisfied: munkres in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (1.1.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 12)) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 12)) (0.22.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 12)) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 12)) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 12)) (2.31.0)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0->-r requirements.txt (line 12))\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 12)) (0.4.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 4)) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 4)) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 4)) (2.9.0.post0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 8)) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 8)) (3.2.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 9)) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 9)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 9)) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 9)) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 9)) (2024.2.0)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->-r requirements.txt (line 16)) (0.2.13)\nRequirement already satisfied: numba!=0.49.0 in /opt/conda/lib/python3.10/site-packages (from pymatting->-r requirements.txt (line 18)) (0.58.1)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba!=0.49.0->pymatting->-r requirements.txt (line 18)) (0.41.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 4)) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 9)) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 12)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 12)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 12)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 12)) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 9)) (1.3.0)\nDownloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\nDownloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tqdm-4.51.0-py2.py3-none-any.whl (70 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\nDownloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading PyMatting-1.1.12-py3-none-any.whl (52 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: wget\n  Building wheel for wget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=215d6ec936dafaa04addc8625936c1ef9fc4ba6984f0b97e3a49679d3e59d4b7\n  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\nSuccessfully built wget\nInstalling collected packages: wget, tokenizers, einops, utils, tqdm, imageio, ftfy, pymatting, transformers, captum\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.66.1\n    Uninstalling tqdm-4.66.1:\n      Successfully uninstalled tqdm-4.66.1\n  Attempting uninstall: imageio\n    Found existing installation: imageio 2.33.1\n    Uninstalling imageio-2.33.1:\n      Successfully uninstalled imageio-2.33.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.39.3\n    Uninstalling transformers-4.39.3:\n      Successfully uninstalled transformers-4.39.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.9.3 requires keras-core, which is not installed.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.6 which is incompatible.\ndatasets 2.18.0 requires tqdm>=4.62.1, but you have tqdm 4.51.0 which is incompatible.\nfitter 1.7.0 requires tqdm<5.0.0,>=4.65.1, but you have tqdm 4.51.0 which is incompatible.\nkaggle-environments 1.14.3 requires transformers>=4.33.1, but you have transformers 4.31.0 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires tqdm>=4.63.0, but you have tqdm 4.51.0 which is incompatible.\npytorch-lightning 2.2.2 requires tqdm>=4.57.0, but you have tqdm 4.51.0 which is incompatible.\nscikit-image 0.22.0 requires imageio>=2.27, but you have imageio 2.9.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.0 requires tqdm>=4.63.0, but you have tqdm 4.51.0 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed captum-0.7.0 einops-0.3.0 ftfy-6.2.0 imageio-2.9.0 pymatting-1.1.12 tokenizers-0.13.3 tqdm-4.51.0 transformers-4.31.0 utils-1.0.1 wget-3.2\n","output_type":"stream"}]},{"cell_type":"code","source":"# !python lxmert/lxmert/perturbation.py --COCO_path /kaggle/input/sample-val2014/ --method \"dsm_grad\" --is-positive-pert True","metadata":{"execution":{"iopub.status.busy":"2024-06-01T07:19:12.314859Z","iopub.execute_input":"2024-06-01T07:19:12.315187Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"%s not found in cache or force_download set to True, downloading to %s https://s3.amazonaws.com/models.huggingface.co/bert/unc-nlp/frcnn-vg-finetuned/config.yaml /root/.cache/torch/transformers/tmpk6pkzwym\nDownloading: 100%|█████████████████████████| 2.13k/2.13k [00:00<00:00, 7.61MB/s]\nloading configuration file cache\n%s not found in cache or force_download set to True, downloading to %s https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin /root/.cache/torch/transformers/tmpfwamb5yh\nDownloading: 100%|███████████████████████████| 262M/262M [00:04<00:00, 53.5MB/s]\nloading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /root/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\nAll model checkpoint weights were used when initializing GeneralizedRCNN.\n\nAll the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\nvocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 6.42MB/s]\nspecial_tokens_map.json: 100%|██████████████████| 112/112 [00:00<00:00, 560kB/s]\ntokenizer_config.json: 100%|████████████████████| 153/153 [00:00<00:00, 919kB/s]\nconfig.json: 100%|█████████████████████████████| 776/776 [00:00<00:00, 5.05MB/s]\nconfig.json: 100%|█████████████████████████████| 880/880 [00:00<00:00, 4.61MB/s]\npytorch_model.bin: 100%|█████████████████████| 856M/856M [00:43<00:00, 19.9MB/s]\nLoad 3303 data from split(s) valid.\nLoad 3303 data from split(s) valid.\n  0%|                                                  | 0/3303 [00:00<?, ?it/s]running positive pert test for image modality with method dsm_grad\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3526.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n/kaggle/working/Transformer-MM-Explainability/lxmert/lxmert/src/ExplanationGenerator.py:528: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/Copy.cpp:299.)\n  eigenvalues, eigenvectors = torch.from_numpy(eigenvalues), torch.from_numpy(eigenvectors.T).float()\nAcc: [66.13, 57.38, 53.49, 49.79, 47.66, 46.46, 43.45, 37.99, 39.41]:  82%|▊| 26","output_type":"stream"}]},{"cell_type":"code","source":"# Acc: [66.62, 58.65, 55.22, 50.45, 48.06, 47.26, 44.81, 38.86, 39.79]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !python lxmert/lxmert/perturbation.py --COCO_path /kaggle/input/sample-val2014/ --method \"partial_lrp\" --is-positive-pert True --is-text-pert True","metadata":{"execution":{"iopub.status.busy":"2024-05-31T13:46:09.915467Z","iopub.execute_input":"2024-05-31T13:46:09.915779Z","iopub.status.idle":"2024-05-31T13:46:09.920350Z","shell.execute_reply.started":"2024-05-31T13:46:09.915746Z","shell.execute_reply":"2024-05-31T13:46:09.919498Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!python lxmert/lxmert/perturbation.py --COCO_path /kaggle/input/sample-val2014/ --method \"dsm_grad\"","metadata":{"execution":{"iopub.status.busy":"2024-06-01T08:50:10.464543Z","iopub.execute_input":"2024-06-01T08:50:10.464957Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"%s not found in cache or force_download set to True, downloading to %s https://s3.amazonaws.com/models.huggingface.co/bert/unc-nlp/frcnn-vg-finetuned/config.yaml /root/.cache/torch/transformers/tmpymyvutd9\nDownloading: 100%|█████████████████████████| 2.13k/2.13k [00:00<00:00, 9.35MB/s]\nloading configuration file cache\n%s not found in cache or force_download set to True, downloading to %s https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin /root/.cache/torch/transformers/tmpn1ix0uji\nDownloading: 100%|███████████████████████████| 262M/262M [00:04<00:00, 54.3MB/s]\nloading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /root/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\nAll model checkpoint weights were used when initializing GeneralizedRCNN.\n\nAll the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\nvocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 1.76MB/s]\nspecial_tokens_map.json: 100%|██████████████████| 112/112 [00:00<00:00, 674kB/s]\ntokenizer_config.json: 100%|████████████████████| 153/153 [00:00<00:00, 977kB/s]\nconfig.json: 100%|█████████████████████████████| 776/776 [00:00<00:00, 5.20MB/s]\nconfig.json: 100%|█████████████████████████████| 880/880 [00:00<00:00, 4.95MB/s]\npytorch_model.bin: 100%|██████████████████████| 856M/856M [00:02<00:00, 310MB/s]\nLoad 3303 data from split(s) valid.\nLoad 3303 data from split(s) valid.\n  0%|                                                  | 0/3303 [00:00<?, ?it/s]running negative pert test for image modality with method dsm_grad\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3526.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n/kaggle/working/Transformer-MM-Explainability/lxmert/lxmert/src/ExplanationGenerator.py:528: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/Copy.cpp:299.)\n  eigenvalues, eigenvectors = torch.from_numpy(eigenvalues), torch.from_numpy(eigenvectors.T).float()\nAcc: [65.43, 64.2, 62.47, 59.51, 56.91, 52.1, 47.65, 40.62, 41.6]:   2%| | 81/33","output_type":"stream"}]},{"cell_type":"code","source":"# !python lxmert/lxmert/perturbation.py --COCO_path /kaggle/input/sample-val2014/ --method \"partial_lrp\" --is-text-pert True","metadata":{"execution":{"iopub.status.busy":"2024-05-31T06:32:18.123575Z","iopub.execute_input":"2024-05-31T06:32:18.124937Z","iopub.status.idle":"2024-05-31T07:53:44.485815Z","shell.execute_reply.started":"2024-05-31T06:32:18.124888Z","shell.execute_reply":"2024-05-31T07:53:44.484644Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"loading configuration file cache\nloading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /root/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\nAll model checkpoint weights were used when initializing GeneralizedRCNN.\n\nAll the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\nLoad 3303 data from split(s) valid.\nLoad 3303 data from split(s) valid.\n  0%|                                                  | 0/3303 [00:00<?, ?it/s]running negative pert test for text modality with method partial_lrp\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3526.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\nAcc: [66.62, 59.84, 51.47, 36.05, 25.87, 20.17, 6.12, 4.55, 4.52]: 100%|█| 3303/\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}