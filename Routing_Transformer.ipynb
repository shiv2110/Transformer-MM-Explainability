{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJuwzIqfDWsp",
        "outputId": "6a05ce06-9917-47e5-eddf-2ec3af334351"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE5SmHg6nRkA",
        "outputId": "38a7638a-5177-49f9-b833-d0023a99d5eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Thesis_2023_24\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Thesis_2023_24/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOVqvcZVn4mT",
        "outputId": "cdac278e-310a-4dba-d25c-0c5ffdf3fcf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting routing_transformer\n",
            "  Downloading routing_transformer-1.6.1-py3-none-any.whl (16 kB)\n",
            "Collecting product-key-memory\n",
            "  Downloading product_key_memory-0.2.10-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: einops in d:\\programs\\anaconda3\\lib\\site-packages (from routing_transformer) (0.3.0)\n",
            "Requirement already satisfied: torch in d:\\programs\\anaconda3\\lib\\site-packages (from routing_transformer) (2.0.1)\n",
            "Collecting local-attention>=1.4.0\n",
            "  Downloading local_attention-1.8.6-py3-none-any.whl (8.1 kB)\n",
            "Collecting mixture-of-experts>=0.2.0\n",
            "  Downloading mixture_of_experts-0.2.3-py3-none-any.whl (6.0 kB)\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "     ---------------------------------------- 42.2/42.2 kB 1.0 MB/s eta 0:00:00\n",
            "Collecting colt5-attention>=0.10.14\n",
            "  Downloading CoLT5_attention-0.10.15-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: filelock in d:\\programs\\anaconda3\\lib\\site-packages (from torch->routing_transformer) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions in d:\\programs\\anaconda3\\lib\\site-packages (from torch->routing_transformer) (4.7.1)\n",
            "Requirement already satisfied: sympy in d:\\programs\\anaconda3\\lib\\site-packages (from torch->routing_transformer) (1.11.1)\n",
            "Requirement already satisfied: networkx in d:\\programs\\anaconda3\\lib\\site-packages (from torch->routing_transformer) (2.8.4)\n",
            "Requirement already satisfied: jinja2 in d:\\programs\\anaconda3\\lib\\site-packages (from torch->routing_transformer) (3.1.2)\n",
            "Requirement already satisfied: packaging in d:\\programs\\anaconda3\\lib\\site-packages (from colt5-attention>=0.10.14->product-key-memory->routing_transformer) (23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\programs\\anaconda3\\lib\\site-packages (from jinja2->torch->routing_transformer) (2.1.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in d:\\programs\\anaconda3\\lib\\site-packages (from sympy->torch->routing_transformer) (1.2.1)\n",
            "Installing collected packages: einops, mixture-of-experts, local-attention, colt5-attention, product-key-memory, routing_transformer\n",
            "  Attempting uninstall: einops\n",
            "    Found existing installation: einops 0.3.0\n",
            "    Uninstalling einops-0.3.0:\n",
            "      Successfully uninstalled einops-0.3.0\n",
            "Successfully installed colt5-attention-0.10.15 einops-0.6.1 local-attention-1.8.6 mixture-of-experts-0.2.3 product-key-memory-0.2.10 routing_transformer-1.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install routing_transformer --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlmURTTxfXot",
        "outputId": "3acad9c4-3333-4e4d-c0bb-22ef5ce960bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: packaging in d:\\programs\\anaconda3\\lib\\site-packages (23.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install packaging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvSmuMS4euCS",
        "outputId": "541e05d4-d11a-4572-d500-363d8f5266db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'apex'...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using pip 22.3.1 from D:\\Programs\\anaconda3\\lib\\site-packages\\pip (python 3.10)\n",
            "Processing d:\\thesis_2023-24\\codes\\transformer-mm-explainability\\apex\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'error'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Implying --no-binary=:all: due to the presence of --build-option / --global-option / --install-option. Consider using --config-settings for more flexibility.\n",
            "DEPRECATION: --no-binary currently disables reading from the cache of locally built wheels. In the future --no-binary will not influence the wheel cache. pip 23.1 will enforce this behaviour change. A possible replacement is to use the --no-cache-dir option. You can use the flag --use-feature=no-binary-enable-wheel-cache to test the upcoming behaviour. Discussion can be found at https://github.com/pypa/pip/issues/11453\n",
            "  Running command pip subprocess to install build dependencies\n",
            "  DEPRECATION: --no-binary currently disables reading from the cache of locally built wheels. In the future --no-binary will not influence the wheel cache. pip 23.1 will enforce this behaviour change. A possible replacement is to use the --no-cache-dir option. You can use the flag --use-feature=no-binary-enable-wheel-cache to test the upcoming behaviour. Discussion can be found at https://github.com/pypa/pip/issues/11453\n",
            "  Collecting setuptools\n",
            "    Downloading setuptools-68.2.2.tar.gz (2.2 MB)\n",
            "       ---------------------------------------- 2.2/2.2 MB 6.7 MB/s eta 0:00:00\n",
            "    Getting requirements to build wheel: started\n",
            "    Getting requirements to build wheel: finished with status 'done'\n",
            "    Installing backend dependencies: started\n",
            "    Installing backend dependencies: finished with status 'done'\n",
            "    Preparing metadata (pyproject.toml): started\n",
            "    Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "  Collecting wheel\n",
            "    Using cached wheel-0.41.2.tar.gz (98 kB)\n",
            "    Installing build dependencies: started\n",
            "    Installing build dependencies: finished with status 'done'\n",
            "    Getting requirements to build wheel: started\n",
            "    Getting requirements to build wheel: finished with status 'done'\n",
            "    Preparing metadata (pyproject.toml): started\n",
            "    Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "  Building wheels for collected packages: wheel, setuptools\n",
            "    Building wheel for wheel (pyproject.toml): started\n",
            "    Building wheel for wheel (pyproject.toml): finished with status 'done'\n",
            "    Created wheel for wheel: filename=wheel-0.41.2-py3-none-any.whl size=64848 sha256=6d719ad52609f41e0d2b03debd273058187a51bb36c7603b93f12312bd0d4792\n",
            "    Stored in directory: c:\\users\\shiva\\appdata\\local\\pip\\cache\\wheels\\d9\\6b\\7a\\263b38798e177479c6b836d5da95ac511f0d8f0b3d37626d8b\n",
            "    Building wheel for setuptools (pyproject.toml): started\n",
            "    Building wheel for setuptools (pyproject.toml): finished with status 'done'\n",
            "    Created wheel for setuptools: filename=setuptools-68.2.2-py3-none-any.whl size=807887 sha256=6ef62288acb33da24c9dd085c788750c87bf03a9bb3d07292542d7783eb5eadd\n",
            "    Stored in directory: c:\\users\\shiva\\appdata\\local\\pip\\cache\\wheels\\3d\\c2\\c9\\91673b75fd99bcded0be6899b22fb61d464f6e93eee25c19e3\n",
            "  Successfully built wheel setuptools\n",
            "  Installing collected packages: wheel, setuptools\n",
            "  ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "  conda-repo-cli 1.0.41 requires requests_mock, which is not installed.\n",
            "  python-lsp-black 1.2.1 requires black>=22.3.0, but you have black 21.4b2 which is incompatible.\n",
            "  conda-repo-cli 1.0.41 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
            "  conda-repo-cli 1.0.41 requires nbformat==5.4.0, but you have nbformat 5.7.0 which is incompatible.\n",
            "  anaconda-client 1.11.2 requires tqdm>=4.56.0, but you have tqdm 4.51.0 which is incompatible.\n",
            "  Successfully installed setuptools-68.2.2 wheel-0.41.2\n",
            "  Running command Getting requirements to build wheel\n",
            "  Traceback (most recent call last):\n",
            "    File \"D:\\Programs\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py\", line 351, in <module>\n",
            "      main()\n",
            "    File \"D:\\Programs\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py\", line 333, in main\n",
            "      json_out['return_val'] = hook(**hook_input['kwargs'])\n",
            "    File \"D:\\Programs\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\n",
            "      return hook(config_settings)\n",
            "    File \"C:\\Users\\shiva\\AppData\\Local\\Temp\\pip-build-env-0gh0_c63\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 355, in get_requires_for_build_wheel\n",
            "      return self._get_build_requires(config_settings, requirements=['wheel'])\n",
            "    File \"C:\\Users\\shiva\\AppData\\Local\\Temp\\pip-build-env-0gh0_c63\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 325, in _get_build_requires\n",
            "      self.run_setup()\n",
            "    File \"C:\\Users\\shiva\\AppData\\Local\\Temp\\pip-build-env-0gh0_c63\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 341, in run_setup\n",
            "      exec(code, locals())\n",
            "    File \"<string>\", line 5, in <module>\n",
            "  ModuleNotFoundError: No module named 'packaging'\n",
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Getting requirements to build wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> See above for output.\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  full command: 'D:\\Programs\\anaconda3\\python.exe' 'D:\\Programs\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py' get_requires_for_build_wheel 'C:\\Users\\shiva\\AppData\\Local\\Temp\\tmpo09bhtn6'\n",
            "  cwd: D:\\Thesis_2023-24\\codes\\Transformer-MM-Explainability\\apex\n",
            "error: subprocess-exited-with-error\n",
            "\n",
            "× Getting requirements to build wheel did not run successfully.\n",
            "│ exit code: 1\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "# %%writefile setup.sh\n",
        "\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" ./apex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV0drV0xfG-y",
        "outputId": "5ea2f2ba-5ab0-43b3-f1c1-434e3cedcfc6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'sh' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "# !sh setup.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "FQevGkxlfJQu",
        "outputId": "d93c6f2b-ceb8-4559-e6e7-b7e58914f95d"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'amp' from 'apex' (unknown location)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32md:\\Thesis_2023-24\\codes\\Transformer-MM-Explainability\\Routing_Transformer.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Thesis_2023-24/codes/Transformer-MM-Explainability/Routing_Transformer.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mapex\u001b[39;00m \u001b[39mimport\u001b[39;00m amp\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Thesis_2023-24/codes/Transformer-MM-Explainability/Routing_Transformer.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Thesis_2023-24/codes/Transformer-MM-Explainability/Routing_Transformer.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrouting_transformer\u001b[39;00m \u001b[39mimport\u001b[39;00m RoutingTransformerLM\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name 'amp' from 'apex' (unknown location)"
          ]
        }
      ],
      "source": [
        "from apex import amp\n",
        "import torch\n",
        "from routing_transformer import RoutingTransformerLM\n",
        "\n",
        "SEQ_LEN = 131072\n",
        "\n",
        "s = RoutingTransformerLM(\n",
        "    num_tokens = 256,\n",
        "    dim = 512,\n",
        "    heads = 8,\n",
        "    depth = 1,\n",
        "    window_size = 128,\n",
        "    reversible = True,\n",
        "    ff_chunks = 40,\n",
        "    max_seq_len = SEQ_LEN\n",
        ").cuda()\n",
        "\n",
        "s = amp.initialize(s, opt_level='O2')\n",
        "\n",
        "x = torch.randint(0, 256, (1, SEQ_LEN)).cuda()\n",
        "y, aux_loss = s(x)\n",
        "y.sum().backward()\n",
        "\n",
        "print('success')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seCPWBgehulk",
        "outputId": "817a053f-665b-4bf3-c5e5-632f3936e0aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1334: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when outputs are generated by different autograd Nodes \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 8853,  1834,   998,  ..., 15881, 10238,  7324]], device='cuda:0')\n",
            "tensor([[True, True, True,  ..., True, True, True]], device='cuda:0')\n",
            "tensor([[[ 0.1505,  0.4908, -0.2077,  ..., -0.1255,  0.8346,  0.5536],\n",
            "         [ 0.9205, -0.0577,  0.2428,  ..., -1.0441,  1.2415,  0.0124],\n",
            "         [ 0.0297, -0.5290,  0.1574,  ..., -0.6887,  1.1949,  0.2831],\n",
            "         ...,\n",
            "         [-0.1320, -0.8083,  0.6914,  ..., -0.6083,  0.1129, -1.0397],\n",
            "         [ 0.1453, -0.6838,  0.9102,  ..., -0.3746,  0.2540, -0.9866],\n",
            "         [ 0.3760, -0.8628,  0.8975,  ..., -0.9756,  0.2434, -1.4790]]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from routing_transformer import RoutingTransformerLM\n",
        "\n",
        "model = RoutingTransformerLM(\n",
        "    num_tokens = 20000,\n",
        "    dim = 512,\n",
        "    heads = 8,\n",
        "    depth = 12,\n",
        "    max_seq_len = 8192,\n",
        "    causal = True,           # auto-regressive or not\n",
        "    emb_dim = 128,           # embedding factorization, from Albert\n",
        "    weight_tie = False,      # weight tie layers, from Albert\n",
        "    tie_embedding = False,   # multiply final embeddings with token weights for logits\n",
        "    dim_head = 64,           # be able to fix the dimension of each head, making it independent of the embedding dimension and the number of heads\n",
        "    attn_dropout = 0.1,      # dropout after attention\n",
        "    attn_layer_dropout = 0., # dropout after self attention layer\n",
        "    ff_dropout = 0.1,        # feedforward dropout\n",
        "    layer_dropout = 0.,      # layer dropout\n",
        "    window_size = 128,       # target window size of each cluster\n",
        "    n_local_attn_heads = 4,  # number of local attention heads\n",
        "    reversible = True,       # reversible networks for memory savings, from Reformer paper\n",
        "    ff_chunks = 10,          # feed forward chunking, from Reformer paper\n",
        "    ff_glu = True,           # use GLU variant in feedforward\n",
        "    pkm_layers = (4, 7),     # specify layers to use product key memory. paper shows 1 or 2 modules near the middle of the transformer is best\n",
        "    pkm_num_keys = 128,      # defaults to 128, but can be increased to 256 or 512 as memory allows\n",
        "    moe_layers = (3, 6),     # specify which layers to use mixture of experts\n",
        "    moe_num_experts = 4,     # number of experts in the mixture of experts layer, defaults to 4. increase for adding more parameters to model\n",
        "    moe_loss_coef = 1e-2,    # the weight for the auxiliary loss in mixture of experts to keep expert usage balanced\n",
        "    num_mem_kv = 8,          # number of memory key/values to append to each cluster of each head, from the 'All-Attention' paper. defaults to 1 in the causal case for unshared QK to work\n",
        "    use_scale_norm = False,  # use scale norm, simplified normalization from 'Transformers without Tears' paper\n",
        "    use_rezero = False,      # use Rezero with no normalization\n",
        "    shift_tokens = True      # shift tokens by one along sequence dimension, for a slight improvement in convergence\n",
        ").cuda()\n",
        "\n",
        "x = torch.randint(0, 20000, (1, 8192)).long().cuda()\n",
        "input_mask = torch.ones_like(x).bool().cuda()\n",
        "\n",
        "y, aux_loss = model(x, input_mask = input_mask) # (1, 8192, 20000)\n",
        "aux_loss.backward() # add auxiliary loss to main loss before backprop\n",
        "\n",
        "print(x)\n",
        "print(input_mask)\n",
        "print(y)\n",
        "print(aux_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfFUtJW8jRTF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
